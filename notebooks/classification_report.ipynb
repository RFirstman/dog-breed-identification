{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification_report.ipynb","provenance":[],"authorship_tag":"ABX9TyPxV+0gfb5L+0tjt33w1mUv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2ffc9a29e8354888a3e11bb7c9d01394":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d9202a317fa42829e21cfe88b3285a6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f483c69b903645909f64a89001abe201","IPY_MODEL_88343e1db3494b17896dd3aa8fa461a3"]}},"4d9202a317fa42829e21cfe88b3285a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f483c69b903645909f64a89001abe201":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f3c3ece55c854af6b9495b51d3041c4f","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f7b11738649435485adcc5802e5e916"}},"88343e1db3494b17896dd3aa8fa461a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_04d4b126296841119637559fb5d9757d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 793583616/? [01:00&lt;00:00, 21285428.74it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9e64683f0cb4e61a73110da00a20734"}},"f3c3ece55c854af6b9495b51d3041c4f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0f7b11738649435485adcc5802e5e916":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04d4b126296841119637559fb5d9757d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9e64683f0cb4e61a73110da00a20734":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"627c8fc5983e4b0f91da1465164f5353":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_577273b937b84e7ba359025782e5d016","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6f67d7e3957a4a5d8c19235fd54c6d54","IPY_MODEL_d2161cdb223141618db4f5f9092391a1"]}},"577273b937b84e7ba359025782e5d016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f67d7e3957a4a5d8c19235fd54c6d54":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4658f0345d084e9eb6d0539ffb74e040","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6b4838f26c8480d96299c59734643e6"}},"d2161cdb223141618db4f5f9092391a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_803b1ab6f2c14b618a4e0f4e74baac9e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 21856256/? [00:23&lt;00:00, 9040040.55it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8be13051823472e9a3e41a632787c28"}},"4658f0345d084e9eb6d0539ffb74e040":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c6b4838f26c8480d96299c59734643e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"803b1ab6f2c14b618a4e0f4e74baac9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e8be13051823472e9a3e41a632787c28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72df6a87ff6f421eb8199835ccef48ec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bf927e6dcf7f48bcb007eb9605232ee7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d574567c51e84b489187ef2c47909cd0","IPY_MODEL_c3b1da35f0054ce9a742be7514312608"]}},"bf927e6dcf7f48bcb007eb9605232ee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d574567c51e84b489187ef2c47909cd0":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_985fa1e16b52429490e9c0669854b004","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ffe2c16b4f7a41578d07a9011e613aa7"}},"c3b1da35f0054ce9a742be7514312608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_abeffb1a5f4445c4894cb60bd4ad693d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483328/? [00:01&lt;00:00, 246927.82it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7be603539dd4415b8a1f73597b306b91"}},"985fa1e16b52429490e9c0669854b004":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ffe2c16b4f7a41578d07a9011e613aa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abeffb1a5f4445c4894cb60bd4ad693d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7be603539dd4415b8a1f73597b306b91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b890a7fb3e9046ca9bc8be1c053c2660":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e2faa6f984384cd581c0b78210a049d2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_68efb2f95d7a45d69c90ae5ed556c9bc","IPY_MODEL_bbc6e01ff3a047989f6879f319412189"]}},"e2faa6f984384cd581c0b78210a049d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68efb2f95d7a45d69c90ae5ed556c9bc":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d2c2d853432842a2ab088c2b15e47e8b","_dom_classes":[],"description":"100%","_model_name":"IntProgressModel","bar_style":"success","max":356082095,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":356082095,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cbee472cafab4a229db932baa265bec9"}},"bbc6e01ff3a047989f6879f319412189":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_41fc34f9181f44c1b5e35f6728f7f90e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 340M/340M [00:18&lt;00:00, 19.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6055c182243247ffb83a79f245518cd1"}},"d2c2d853432842a2ab088c2b15e47e8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cbee472cafab4a229db932baa265bec9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"41fc34f9181f44c1b5e35f6728f7f90e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6055c182243247ffb83a79f245518cd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Ri6Fzqd_xF4V","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()   # interactive mode\n","# cuda = torch.device('cuda')\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzOXyZBexKvU","colab_type":"code","colab":{}},"source":["data_dir = \"data/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oI_kRKY7xUNg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":304,"referenced_widgets":["2ffc9a29e8354888a3e11bb7c9d01394","4d9202a317fa42829e21cfe88b3285a6","f483c69b903645909f64a89001abe201","88343e1db3494b17896dd3aa8fa461a3","f3c3ece55c854af6b9495b51d3041c4f","0f7b11738649435485adcc5802e5e916","04d4b126296841119637559fb5d9757d","f9e64683f0cb4e61a73110da00a20734","627c8fc5983e4b0f91da1465164f5353","577273b937b84e7ba359025782e5d016","6f67d7e3957a4a5d8c19235fd54c6d54","d2161cdb223141618db4f5f9092391a1","4658f0345d084e9eb6d0539ffb74e040","c6b4838f26c8480d96299c59734643e6","803b1ab6f2c14b618a4e0f4e74baac9e","e8be13051823472e9a3e41a632787c28","72df6a87ff6f421eb8199835ccef48ec","bf927e6dcf7f48bcb007eb9605232ee7","d574567c51e84b489187ef2c47909cd0","c3b1da35f0054ce9a742be7514312608","985fa1e16b52429490e9c0669854b004","ffe2c16b4f7a41578d07a9011e613aa7","abeffb1a5f4445c4894cb60bd4ad693d","7be603539dd4415b8a1f73597b306b91"]},"outputId":"c1e807bf-d433-49ba-fbe9-05d70eef5003","executionInfo":{"status":"ok","timestamp":1588295249264,"user_tz":240,"elapsed":57231,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["from data.load_data import load_datasets\n","train_dataset, val_dataset, test_dataset, classes = load_datasets(data_dir)\n","class_names = train_dataset.classes\n","print(len(class_names))\n","\n","dataset_sizes = {\"train\": len(train_dataset), \"val\": len(val_dataset)}"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar to data/StanfordDogsData/images.tar\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ffc9a29e8354888a3e11bb7c9d01394","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar to data/StanfordDogsData/annotation.tar\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"627c8fc5983e4b0f91da1465164f5353","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Downloading http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar to data/StanfordDogsData/lists.tar\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72df6a87ff6f421eb8199835ccef48ec","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Training set stats:\n","9600 samples spanning 120 classes (avg 80.000000 per class)\n","Validation set stats:\n","2400 samples spanning 120 classes (avg 20.000000 per class)\n","Testing set stats:\n","8580 samples spanning 120 classes (avg 71.500000 per class)\n","120\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"noeCofi_xVQ3","colab_type":"code","colab":{}},"source":["kwargs = {'num_workers': 1, 'pin_memory': True}\n","batch_size = 32\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","dataloaders = {\"train\": train_loader, \"val\": val_loader}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sO2MrCWAxXGC","colab_type":"code","colab":{}},"source":["def evaluate(model, optimizer, criterion):\n","    model.eval()\n","    running_loss = 0\n","    running_corrects = 0\n","    n_examples = 0\n","    with torch.no_grad():\n","        \n","        model.eval()   # Set model to evaluate mode\n","\n","        for inputs, labels in test_loader:\n","        \n","          inputs = inputs.to(device)\n","          # inputs = inputs.cuda()\n","          labels = labels.to(device)\n","          # labels = labels.cuda()\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          # forward\n","          # track history if only in train\n","          with torch.set_grad_enabled(False):\n","              outputs = model(inputs)\n","              _, preds = torch.max(outputs, 1)\n","              loss = criterion(outputs, labels)\n","\n","          # statistics\n","          running_loss += loss.item() * inputs.size(0)\n","          running_corrects += torch.sum(preds == labels.data)\n","          n_examples += preds.size(0)\n","\n","    epoch_loss = running_loss / len(test_dataset)\n","    epoch_acc = 100. * running_corrects.double() / len(test_dataset)\n","    running_loss /= n_examples\n","    return running_loss, running_corrects, n_examples, epoch_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2AnRwnwxZCE","colab_type":"code","colab":{}},"source":["def train_model(model, model_name, criterion, optimizer, scheduler, hp_info, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    file_name = \"{model_name}.log\"\n","    file_name = file_name.format(model_name=model_name, lr=hp_info['lr'], momentum=hp_info['momentum'])\n","    log_file = open(file_name, 'w')\n","    \n","    print('-'*10)\n","    print('learning rate: {}, momentum: {}'.format(hp_info['lr'], hp_info['momentum']))\n","\n","    for epoch in range(num_epochs):\n","        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        # print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        log_file_string = \"\"\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                # inputs = inputs.cuda()\n","                labels = labels.to(device)\n","                # labels = labels.cuda()\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            \n","            if phase == 'train':\n","                log_file_string = 'Train Epoch: {}\\t Train Loss: {:.6f}\\t Train Acc:{}\\t '.format(epoch, epoch_loss, epoch_acc)\n","            else:\n","                log_file_string += 'Val Loss: {}\\t Val Acc: {}\\n'.format(epoch_loss, epoch_acc)\n","                log_file.write(log_file_string)\n","                print(log_file_string)\n","            # log_file.write(log_file_string)\n","            # print(log_file_string)\n","\n","            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","            #     phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        # print()\n","\n","    time_elapsed = time.time() - since\n","    # print('Training complete in {:.0f}m {:.0f}s'.format(\n","    #     time_elapsed // 60, time_elapsed % 60))\n","    # print('Best val Acc: {:4f}'.format(best_acc))  \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    test_loss, test_correct, test_n_examples, test_acc = evaluate(model, optimizer, criterion)\n","    log_file_string = '\\ntest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n","    log_file_string = log_file_string.format(test_loss, test_correct, test_n_examples, test_acc)\n","    log_file.write(log_file_string)\n","    print(log_file_string)\n","\n","    log_file.close()\n","\n","    return model, test_acc, best_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7XbBxCFtxa6v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["b890a7fb3e9046ca9bc8be1c053c2660","e2faa6f984384cd581c0b78210a049d2","68efb2f95d7a45d69c90ae5ed556c9bc","bbc6e01ff3a047989f6879f319412189","d2c2d853432842a2ab088c2b15e47e8b","cbee472cafab4a229db932baa265bec9","41fc34f9181f44c1b5e35f6728f7f90e","6055c182243247ffb83a79f245518cd1"]},"outputId":"a4815039-3ace-4a27-f814-3d1f16a587d0","executionInfo":{"status":"ok","timestamp":1588295270573,"user_tz":240,"elapsed":21235,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["lr = 0.01\n","mom = 0.9\n","\n","model_type = \"resnext\"\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","model_conv = models.resnext101_32x8d(pretrained=True)\n","\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","\n","# model_conv.fc = nn.Linear(num_ftrs, 120)\n","model_conv.fc = nn.Sequential(nn.Linear(num_ftrs, 512), nn.Linear(512, 512), nn.Linear(512, 120))\n","\n","model_conv = model_conv.to(device)\n","\n","# Observe that only parameters of final layer are being optimized as\n","# opposed to before.\n","params = model_conv.fc.parameters()\n","optimizer_conv = optim.SGD(params, lr=lr, momentum=mom)\n","\n","# Decay LR by a factor of 0.1 every 5 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n","hp_info = {'lr': lr, 'momentum': mom}\n","# my_models.append({'model': model_conv, 'optimizer': optimizer_conv, 'exp_lr_scheduler': exp_lr_scheduler, 'hp_info': hp_info})"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/checkpoints/resnext101_32x8d-8ba56ff5.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b890a7fb3e9046ca9bc8be1c053c2660","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=356082095), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8C9zVY4Exdr2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":819},"outputId":"8c358528-e245-431e-c7db-b6e37a6c5036","executionInfo":{"status":"ok","timestamp":1588297326097,"user_tz":240,"elapsed":2076756,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["model_conv, test_acc, val_acc = train_model(model_conv,\n","                                        model_type, \n","                                        criterion, \n","                                        optimizer_conv,\n","                                        exp_lr_scheduler, \n","                                        hp_info, \n","                                        num_epochs=20)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["----------\n","learning rate: 0.01, momentum: 0.9\n","Train Epoch: 0\t Train Loss: 2.079241\t Train Acc:0.535\t Val Loss: 0.8304375346501668\t Val Acc: 0.7612500000000001\n","\n","Train Epoch: 1\t Train Loss: 0.852193\t Train Acc:0.7567708333333334\t Val Loss: 0.762613662481308\t Val Acc: 0.7837500000000001\n","\n","Train Epoch: 2\t Train Loss: 0.791131\t Train Acc:0.7835416666666667\t Val Loss: 0.759389332930247\t Val Acc: 0.7908333333333334\n","\n","Train Epoch: 3\t Train Loss: 0.753601\t Train Acc:0.7889583333333334\t Val Loss: 0.6980640695492426\t Val Acc: 0.8\n","\n","Train Epoch: 4\t Train Loss: 0.729628\t Train Acc:0.7961458333333333\t Val Loss: 0.8308371857802073\t Val Acc: 0.7791666666666667\n","\n","Train Epoch: 5\t Train Loss: 0.699910\t Train Acc:0.7973958333333334\t Val Loss: 0.7728929396470388\t Val Acc: 0.78625\n","\n","Train Epoch: 6\t Train Loss: 0.669006\t Train Acc:0.8119791666666667\t Val Loss: 0.7057832290728887\t Val Acc: 0.81\n","\n","Train Epoch: 7\t Train Loss: 0.533072\t Train Acc:0.8522916666666667\t Val Loss: 0.6373159343004227\t Val Acc: 0.8270833333333334\n","\n","Train Epoch: 8\t Train Loss: 0.501971\t Train Acc:0.8564583333333334\t Val Loss: 0.6109518297513326\t Val Acc: 0.8433333333333334\n","\n","Train Epoch: 9\t Train Loss: 0.511158\t Train Acc:0.8579166666666668\t Val Loss: 0.6034916184345881\t Val Acc: 0.8304166666666667\n","\n","Train Epoch: 10\t Train Loss: 0.471086\t Train Acc:0.8662500000000001\t Val Loss: 0.5755702316761017\t Val Acc: 0.8383333333333334\n","\n","Train Epoch: 11\t Train Loss: 0.500532\t Train Acc:0.8615625\t Val Loss: 0.6159631087382634\t Val Acc: 0.8283333333333334\n","\n","Train Epoch: 12\t Train Loss: 0.463426\t Train Acc:0.8700000000000001\t Val Loss: 0.5672750455141068\t Val Acc: 0.8416666666666667\n","\n","Train Epoch: 13\t Train Loss: 0.490622\t Train Acc:0.8606250000000001\t Val Loss: 0.6228818303346634\t Val Acc: 0.8304166666666667\n","\n","Train Epoch: 14\t Train Loss: 0.476164\t Train Acc:0.8635416666666668\t Val Loss: 0.5550874914725622\t Val Acc: 0.8445833333333334\n","\n","Train Epoch: 15\t Train Loss: 0.454394\t Train Acc:0.8732291666666667\t Val Loss: 0.5522787111997605\t Val Acc: 0.84125\n","\n","Train Epoch: 16\t Train Loss: 0.458216\t Train Acc:0.8727083333333334\t Val Loss: 0.5889233024915059\t Val Acc: 0.8387500000000001\n","\n","Train Epoch: 17\t Train Loss: 0.445074\t Train Acc:0.8754166666666667\t Val Loss: 0.620462078054746\t Val Acc: 0.8220833333333334\n","\n","Train Epoch: 18\t Train Loss: 0.483231\t Train Acc:0.8618750000000001\t Val Loss: 0.6163708690802256\t Val Acc: 0.8275\n","\n","Train Epoch: 19\t Train Loss: 0.452898\t Train Acc:0.8711458333333334\t Val Loss: 0.5772792456547419\t Val Acc: 0.8354166666666667\n","\n","\n","test set: Average loss: 0.5864, Accuracy: 7208/8580 (84%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rpiw5Ni_33gZ","colab_type":"code","colab":{}},"source":["import model_evaluation_utils as meu"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCp0JAkX5s3r","colab_type":"code","colab":{}},"source":["test_labels = []\n","test_predictions = []\n","for inputs, labels in test_loader:\n","  test_labels.extend(labels.numpy())\n","  inputs = inputs.to(device)\n","  outputs = model_conv(inputs)\n","  _, preds = torch.max(outputs, 1)\n","  test_predictions.extend(preds.cpu().numpy())\n","\n","assert len(test_labels) == len(test_predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hD7HUikZ7sHz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"ef1c847c-4ef9-49a9-d0e3-c8cb52957252","executionInfo":{"status":"ok","timestamp":1588298769468,"user_tz":240,"elapsed":1223,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["meu.get_metrics(true_labels=test_labels, predicted_labels=test_predictions)"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Accuracy: 0.8442\n","Precision: 0.8494\n","Recall: 0.8442\n","F1 Score: 0.8452\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e0WQ9dtb8v8Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"outputId":"5d02ff1b-14be-4298-ea2d-e0552442f3c6","executionInfo":{"status":"ok","timestamp":1588299080728,"user_tz":240,"elapsed":920,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["data_labels = pd.read_csv('labels.csv')\n","target_labels = data_labels['breed']\n","# pd.get_dummies(target_labels, sparse=True)\n","target_labels"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0                     boston_bull\n","1                           dingo\n","2                        pekinese\n","3                        bluetick\n","4                golden_retriever\n","                   ...           \n","10217                      borzoi\n","10218              dandie_dinmont\n","10219                    airedale\n","10220          miniature_pinscher\n","10221    chesapeake_bay_retriever\n","Name: breed, Length: 10222, dtype: object"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"5VA9LNie89SL","colab_type":"code","colab":{}},"source":["from data.dog_classes import classes\n","import pandas as pd\n","# class_labels = pd.DataFrame(classes)\n","class_labels = classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEVAxp8u9sbQ","colab_type":"code","colab":{}},"source":["test_labels_class_names = []\n","for label in test_labels:\n","  test_labels_class_names.append(class_labels[label])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"84WNzJhoBFXz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"outputId":"7dc09630-1ca1-43a3-96be-b333c2ca855d","executionInfo":{"status":"ok","timestamp":1588299325063,"user_tz":240,"elapsed":911,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["pd.get_dummies(test_labels_class_names, sparse=True)"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Affenpinscher</th>\n","      <th>Afghan Hound</th>\n","      <th>African Hunting Dog</th>\n","      <th>Airedale</th>\n","      <th>American Staffordshire Terrier</th>\n","      <th>Appenzeller</th>\n","      <th>Australian Terrier</th>\n","      <th>Basenji</th>\n","      <th>Basset Hound</th>\n","      <th>Beagle</th>\n","      <th>Bedlington Terrier</th>\n","      <th>Bernese Mountain Dog</th>\n","      <th>Black-and-tan Coonhound</th>\n","      <th>Blenheim Spaniel</th>\n","      <th>Bloodhound</th>\n","      <th>Bluetick</th>\n","      <th>Border Collie</th>\n","      <th>Border Terrier</th>\n","      <th>Borzoi</th>\n","      <th>Boston Bull</th>\n","      <th>Bouvier des Flandres</th>\n","      <th>Boxer</th>\n","      <th>Brabancon Griffon</th>\n","      <th>Briard</th>\n","      <th>Brittany</th>\n","      <th>Bull Mastiff</th>\n","      <th>Cairn</th>\n","      <th>Cardigan</th>\n","      <th>Chesapeake Bay Retriever</th>\n","      <th>Chihuaha</th>\n","      <th>Chow</th>\n","      <th>Clumber</th>\n","      <th>Cocker Spaniel</th>\n","      <th>Collie</th>\n","      <th>Curly-coater Retriever</th>\n","      <th>Dandi Dinmont</th>\n","      <th>Dhole</th>\n","      <th>Dingo</th>\n","      <th>Doberman</th>\n","      <th>English Foxhound</th>\n","      <th>...</th>\n","      <th>Norwegian Elkhound</th>\n","      <th>Norwich Terrier</th>\n","      <th>Old English Sheepdog</th>\n","      <th>Otterhound</th>\n","      <th>Papillon</th>\n","      <th>Pekinese</th>\n","      <th>Pembroke</th>\n","      <th>Pomeranian</th>\n","      <th>Pug</th>\n","      <th>Redbone</th>\n","      <th>Rhodesian Ridgeback</th>\n","      <th>Rottweiler</th>\n","      <th>Saint Bernard</th>\n","      <th>Saluki</th>\n","      <th>Samoyed</th>\n","      <th>Schipperke</th>\n","      <th>Scotch Terrier</th>\n","      <th>Scottish Deerhound</th>\n","      <th>Sealyham Terrier</th>\n","      <th>Shetland Sheepdog</th>\n","      <th>Shih-Tzu</th>\n","      <th>Siberian Husky</th>\n","      <th>Silky Terrier</th>\n","      <th>Soft-coated Wheaten Terrier</th>\n","      <th>Staffordshire Bullterrier</th>\n","      <th>Standard Poodle</th>\n","      <th>Standard Schnauzer</th>\n","      <th>Sussex Spaniel</th>\n","      <th>Tibetan Mastiff</th>\n","      <th>Tibetan Terrier</th>\n","      <th>Toy Poodle</th>\n","      <th>Toy Terrier</th>\n","      <th>Vizsla</th>\n","      <th>Walker Hound</th>\n","      <th>Weimaraner</th>\n","      <th>Welsh Springer Spaniel</th>\n","      <th>West Highland White Terrier</th>\n","      <th>Whippet</th>\n","      <th>Wirehaired Fox Terrier</th>\n","      <th>Yorkshire Terrier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8575</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8576</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8577</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8578</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8579</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8580 rows × 120 columns</p>\n","</div>"],"text/plain":["      Affenpinscher  Afghan Hound  ...  Wirehaired Fox Terrier  Yorkshire Terrier\n","0                 0             0  ...                       0                  0\n","1                 0             0  ...                       0                  0\n","2                 0             0  ...                       0                  0\n","3                 0             0  ...                       0                  0\n","4                 0             0  ...                       0                  0\n","...             ...           ...  ...                     ...                ...\n","8575              0             0  ...                       0                  0\n","8576              0             0  ...                       0                  0\n","8577              0             0  ...                       0                  0\n","8578              0             0  ...                       0                  0\n","8579              0             0  ...                       0                  0\n","\n","[8580 rows x 120 columns]"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"p2PVdqRH7ynv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"9aff70b8-0105-49f1-d679-200aa9913d8b","executionInfo":{"status":"ok","timestamp":1588301078625,"user_tz":240,"elapsed":904,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["# print(test_labels)\n","# print(test_predictions)\n","# print(class_labels)\n","# meu.display_classification_report(true_labels=test_labels, \n","#                                   predicted_labels=test_predictions, \n","#                                   classes=class_labels)\n","from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(test_labels, test_predictions)"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 45,   0,   0, ...,   0,   0,   0],\n","       [  1,  70,   0, ...,   0,   0,   0],\n","       [  0,   1, 129, ...,   0,   0,   0],\n","       ...,\n","       [  0,   0,   0, ...,  47,   1,   0],\n","       [  0,   0,   0, ...,   2,  45,   2],\n","       [  0,   0,   0, ...,   0,   1,  67]])"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"sb6zTA4RCwXf","colab_type":"code","colab":{}},"source":["from sklearn.metrics import classification_report\n","report = classification_report(test_labels, test_predictions, target_names=class_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KK-LuzWaDHl-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"11058bd1-d77f-4509-9d1a-ac7aa60b8755","executionInfo":{"status":"ok","timestamp":1588299908861,"user_tz":240,"elapsed":842,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["print(report)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["                                precision    recall  f1-score   support\n","\n","                      Chihuaha       0.71      0.87      0.78        52\n","              Japanese Spaniel       0.88      0.82      0.85        85\n","                   Maltese Dog       0.96      0.85      0.90       152\n","                      Pekinese       0.88      0.86      0.87        49\n","                      Shih-Tzu       0.79      0.77      0.78       114\n","              Blenheim Spaniel       0.91      0.89      0.90        88\n","                      Papillon       0.91      0.93      0.92        96\n","                   Toy Terrier       0.78      0.83      0.81        72\n","           Rhodesian Ridgeback       0.84      0.74      0.79        72\n","                  Afghan Hound       0.98      0.90      0.94       139\n","                  Basset Hound       0.83      0.92      0.87        75\n","                        Beagle       0.88      0.80      0.84        95\n","                    Bloodhound       0.90      0.85      0.88        87\n","                      Bluetick       0.90      0.85      0.87        71\n","       Black-and-tan Coonhound       0.78      0.80      0.79        59\n","                  Walker Hound       0.50      0.62      0.55        53\n","              English Foxhound       0.76      0.72      0.74        57\n","                       Redbone       0.65      0.81      0.72        48\n","                        Borzoi       0.82      0.92      0.87        51\n","               Irish Wolfhound       0.86      0.81      0.83       118\n","             Italian Greyhound       0.78      0.89      0.83        82\n","                       Whippet       0.86      0.79      0.83        87\n","                 Ibizian Hound       0.89      0.90      0.89        88\n","            Norwegian Elkhound       0.90      0.89      0.89        96\n","                    Otterhound       0.75      0.88      0.81        51\n","                        Saluki       0.94      0.87      0.90       100\n","            Scottish Deerhound       0.94      0.89      0.92       132\n","                    Weimaraner       0.89      0.90      0.89        60\n","     Staffordshire Bullterrier       0.65      0.71      0.68        55\n","American Staffordshire Terrier       0.76      0.69      0.72        64\n","            Bedlington Terrier       0.93      0.85      0.89        82\n","                Border Terrier       0.86      0.92      0.89        72\n","            Kerry Blue Terrier       0.89      0.86      0.88        79\n","                 Irish Terrier       0.89      0.80      0.84        69\n","               Norfolk Terrier       0.71      0.78      0.74        72\n","               Norwich Terrier       0.80      0.81      0.81        85\n","             Yorkshire Terrier       0.74      0.77      0.75        64\n","        Wirehaired Fox Terrier       0.71      0.82      0.76        57\n","              Lakeland Terrier       0.85      0.82      0.84        97\n","              Sealyham Terrier       0.96      0.89      0.92       102\n","                      Airedale       0.89      0.82      0.86       102\n","                         Cairn       0.91      0.93      0.92        97\n","            Australian Terrier       0.85      0.76      0.80        96\n","                 Dandi Dinmont       0.89      0.89      0.89        80\n","                   Boston Bull       0.91      0.83      0.87        82\n","           Miniature Schnauzer       0.82      0.83      0.83        54\n","               Giant Schnauzer       0.79      0.74      0.76        57\n","            Standard Schnauzer       0.71      0.71      0.71        55\n","                Scotch Terrier       0.85      0.76      0.80        58\n","               Tibetan Terrier       0.84      0.82      0.83       106\n","                 Silky Terrier       0.90      0.83      0.86        83\n","   Soft-coated Wheaten Terrier       0.84      0.77      0.80        56\n","   West Highland White Terrier       0.83      0.84      0.83        69\n","                         Lhasa       0.71      0.71      0.71        86\n","         Flat-coated Retriever       0.76      0.81      0.79        52\n","        Curly-coater Retriever       0.87      0.92      0.90        51\n","              Golden Retriever       0.87      0.82      0.85        50\n","            Labrador Retriever       0.89      0.83      0.86        71\n","      Chesapeake Bay Retriever       0.89      0.85      0.87        67\n","   German Short-haired Pointer       0.85      0.85      0.85        52\n","                        Vizsla       0.79      0.93      0.85        54\n","                English Setter       0.87      0.90      0.89        61\n","                  Irish Setter       0.90      0.95      0.92        55\n","                 Gordon Setter       0.90      0.89      0.90        53\n","                      Brittany       0.81      0.85      0.83        52\n","                       Clumber       0.86      0.86      0.86        50\n","      English Springer Spaniel       0.86      0.85      0.85        59\n","        Welsh Springer Spaniel       0.84      0.76      0.80        50\n","                Cocker Spaniel       0.88      0.90      0.89        59\n","                Sussex Spaniel       0.92      0.90      0.91        51\n","           Irish Water Spaniel       0.88      0.90      0.89        50\n","                        Kuvasz       0.72      0.82      0.77        50\n","                    Schipperke       0.89      0.93      0.91        54\n","                   Groenendael       0.83      0.86      0.84        50\n","                      Malinois       0.77      0.96      0.86        50\n","                        Briard       0.79      0.81      0.80        52\n","                        Kelpie       0.75      0.81      0.78        53\n","                      Komondor       0.87      0.89      0.88        54\n","          Old English Sheepdog       0.97      0.87      0.92        69\n","             Shetland Sheepdog       0.84      0.89      0.86        57\n","                        Collie       0.80      0.62      0.70        53\n","                 Border Collie       0.66      0.86      0.75        50\n","          Bouvier des Flandres       0.72      0.82      0.77        50\n","                    Rottweiler       0.83      0.94      0.88        52\n","                German Shepard       0.84      0.73      0.78        52\n","                      Doberman       0.85      0.80      0.82        50\n","            Miniature Pinscher       0.92      0.81      0.86        84\n","    Greater Swiss Mountain Dog       0.81      0.85      0.83        68\n","          Bernese Mountain Dog       0.95      0.92      0.93       118\n","                   Appenzeller       0.71      0.71      0.71        51\n","                   EntleBucher       0.89      0.83      0.86       102\n","                         Boxer       0.78      0.90      0.84        51\n","                  Bull Mastiff       0.89      0.84      0.86        56\n","               Tibetan Mastiff       0.89      0.92      0.91        52\n","                French Bulldog       0.78      0.83      0.80        59\n","                    Great Dane       0.92      0.82      0.87        56\n","                 Saint Bernard       0.97      0.91      0.94        70\n","                    Eskimo Dog       0.39      0.56      0.46        50\n","                      Malamute       0.85      0.81      0.83        78\n","                Siberian Husky       0.79      0.64      0.71        92\n","                 Affenpinscher       0.84      0.98      0.91        50\n","                       Basenji       0.88      0.90      0.89       109\n","                           Pug       0.88      0.90      0.89       100\n","                      Leonberg       0.96      0.95      0.96       110\n","                  Newfoundland       0.86      0.85      0.86        95\n","                Great Pyrenees       0.86      0.84      0.85       113\n","                       Samoyed       0.95      0.92      0.93       118\n","                    Pomeranian       0.90      0.87      0.89       119\n","                          Chow       0.86      0.98      0.92        96\n","                      Keeshond       0.92      1.00      0.96        58\n","             Brabancon Griffon       0.86      0.92      0.89        53\n","                      Pembroke       0.93      0.86      0.90        81\n","                      Cardigan       0.74      0.82      0.78        55\n","                    Toy Poodle       0.71      0.69      0.70        51\n","              Miniature Poodle       0.58      0.67      0.62        55\n","               Standard Poodle       0.76      0.80      0.78        59\n","              Mexican Hairless       0.91      0.95      0.93        55\n","                         Dingo       0.73      0.84      0.78        56\n","                         Dhole       0.90      0.90      0.90        50\n","           African Hunting Dog       0.91      0.97      0.94        69\n","\n","                      accuracy                           0.84      8580\n","                     macro avg       0.84      0.84      0.84      8580\n","                  weighted avg       0.85      0.84      0.85      8580\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QmvR2Cz6Diby","colab_type":"code","colab":{}},"source":["with open(\"classification_report.txt\", \"w\") as f:\n","  f.write(report)\n","  f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMgBZcj3DRAT","colab_type":"code","colab":{}},"source":["report_dict = classification_report(test_labels, test_predictions, target_names=class_labels, output_dict=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11ebdSptDVeh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0915beda-62a5-47bc-f950-fe3257e9806f","executionInfo":{"status":"ok","timestamp":1588299926309,"user_tz":240,"elapsed":672,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["report_dict"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Affenpinscher': {'f1-score': 0.9074074074074074,\n","  'precision': 0.8448275862068966,\n","  'recall': 0.98,\n","  'support': 50},\n"," 'Afghan Hound': {'f1-score': 0.9398496240601504,\n","  'precision': 0.984251968503937,\n","  'recall': 0.8992805755395683,\n","  'support': 139},\n"," 'African Hunting Dog': {'f1-score': 0.937062937062937,\n","  'precision': 0.9054054054054054,\n","  'recall': 0.9710144927536232,\n","  'support': 69},\n"," 'Airedale': {'f1-score': 0.8571428571428571,\n","  'precision': 0.8936170212765957,\n","  'recall': 0.8235294117647058,\n","  'support': 102},\n"," 'American Staffordshire Terrier': {'f1-score': 0.7213114754098361,\n","  'precision': 0.7586206896551724,\n","  'recall': 0.6875,\n","  'support': 64},\n"," 'Appenzeller': {'f1-score': 0.7058823529411765,\n","  'precision': 0.7058823529411765,\n","  'recall': 0.7058823529411765,\n","  'support': 51},\n"," 'Australian Terrier': {'f1-score': 0.802197802197802,\n","  'precision': 0.8488372093023255,\n","  'recall': 0.7604166666666666,\n","  'support': 96},\n"," 'Basenji': {'f1-score': 0.8909090909090909,\n","  'precision': 0.8828828828828829,\n","  'recall': 0.8990825688073395,\n","  'support': 109},\n"," 'Basset Hound': {'f1-score': 0.8734177215189873,\n","  'precision': 0.8313253012048193,\n","  'recall': 0.92,\n","  'support': 75},\n"," 'Beagle': {'f1-score': 0.839779005524862,\n","  'precision': 0.8837209302325582,\n","  'recall': 0.8,\n","  'support': 95},\n"," 'Bedlington Terrier': {'f1-score': 0.8917197452229298,\n","  'precision': 0.9333333333333333,\n","  'recall': 0.8536585365853658,\n","  'support': 82},\n"," 'Bernese Mountain Dog': {'f1-score': 0.9310344827586206,\n","  'precision': 0.9473684210526315,\n","  'recall': 0.9152542372881356,\n","  'support': 118},\n"," 'Black-and-tan Coonhound': {'f1-score': 0.7899159663865546,\n","  'precision': 0.7833333333333333,\n","  'recall': 0.7966101694915254,\n","  'support': 59},\n"," 'Blenheim Spaniel': {'f1-score': 0.896551724137931,\n","  'precision': 0.9069767441860465,\n","  'recall': 0.8863636363636364,\n","  'support': 88},\n"," 'Bloodhound': {'f1-score': 0.8757396449704142,\n","  'precision': 0.9024390243902439,\n","  'recall': 0.8505747126436781,\n","  'support': 87},\n"," 'Bluetick': {'f1-score': 0.8695652173913043,\n","  'precision': 0.8955223880597015,\n","  'recall': 0.8450704225352113,\n","  'support': 71},\n"," 'Border Collie': {'f1-score': 0.7478260869565216,\n","  'precision': 0.6615384615384615,\n","  'recall': 0.86,\n","  'support': 50},\n"," 'Border Terrier': {'f1-score': 0.8859060402684563,\n","  'precision': 0.8571428571428571,\n","  'recall': 0.9166666666666666,\n","  'support': 72},\n"," 'Borzoi': {'f1-score': 0.8703703703703703,\n","  'precision': 0.8245614035087719,\n","  'recall': 0.9215686274509803,\n","  'support': 51},\n"," 'Boston Bull': {'f1-score': 0.8662420382165605,\n","  'precision': 0.9066666666666666,\n","  'recall': 0.8292682926829268,\n","  'support': 82},\n"," 'Bouvier des Flandres': {'f1-score': 0.7663551401869159,\n","  'precision': 0.7192982456140351,\n","  'recall': 0.82,\n","  'support': 50},\n"," 'Boxer': {'f1-score': 0.8363636363636364,\n","  'precision': 0.7796610169491526,\n","  'recall': 0.9019607843137255,\n","  'support': 51},\n"," 'Brabancon Griffon': {'f1-score': 0.8909090909090909,\n","  'precision': 0.8596491228070176,\n","  'recall': 0.9245283018867925,\n","  'support': 53},\n"," 'Briard': {'f1-score': 0.7999999999999999,\n","  'precision': 0.7924528301886793,\n","  'recall': 0.8076923076923077,\n","  'support': 52},\n"," 'Brittany': {'f1-score': 0.830188679245283,\n","  'precision': 0.8148148148148148,\n","  'recall': 0.8461538461538461,\n","  'support': 52},\n"," 'Bull Mastiff': {'f1-score': 0.8623853211009174,\n","  'precision': 0.8867924528301887,\n","  'recall': 0.8392857142857143,\n","  'support': 56},\n"," 'Cairn': {'f1-score': 0.9183673469387756,\n","  'precision': 0.9090909090909091,\n","  'recall': 0.9278350515463918,\n","  'support': 97},\n"," 'Cardigan': {'f1-score': 0.7758620689655172,\n","  'precision': 0.7377049180327869,\n","  'recall': 0.8181818181818182,\n","  'support': 55},\n"," 'Chesapeake Bay Retriever': {'f1-score': 0.8702290076335878,\n","  'precision': 0.890625,\n","  'recall': 0.8507462686567164,\n","  'support': 67},\n"," 'Chihuaha': {'f1-score': 0.7826086956521738,\n","  'precision': 0.7142857142857143,\n","  'recall': 0.8653846153846154,\n","  'support': 52},\n"," 'Chow': {'f1-score': 0.9170731707317074,\n","  'precision': 0.8623853211009175,\n","  'recall': 0.9791666666666666,\n","  'support': 96},\n"," 'Clumber': {'f1-score': 0.8599999999999999,\n","  'precision': 0.86,\n","  'recall': 0.86,\n","  'support': 50},\n"," 'Cocker Spaniel': {'f1-score': 0.8907563025210085,\n","  'precision': 0.8833333333333333,\n","  'recall': 0.8983050847457628,\n","  'support': 59},\n"," 'Collie': {'f1-score': 0.7021276595744681,\n","  'precision': 0.8048780487804879,\n","  'recall': 0.6226415094339622,\n","  'support': 53},\n"," 'Curly-coater Retriever': {'f1-score': 0.8952380952380952,\n","  'precision': 0.8703703703703703,\n","  'recall': 0.9215686274509803,\n","  'support': 51},\n"," 'Dandi Dinmont': {'f1-score': 0.8875,\n","  'precision': 0.8875,\n","  'recall': 0.8875,\n","  'support': 80},\n"," 'Dhole': {'f1-score': 0.9, 'precision': 0.9, 'recall': 0.9, 'support': 50},\n"," 'Dingo': {'f1-score': 0.7833333333333332,\n","  'precision': 0.734375,\n","  'recall': 0.8392857142857143,\n","  'support': 56},\n"," 'Doberman': {'f1-score': 0.8247422680412372,\n","  'precision': 0.851063829787234,\n","  'recall': 0.8,\n","  'support': 50},\n"," 'English Foxhound': {'f1-score': 0.7387387387387387,\n","  'precision': 0.7592592592592593,\n","  'recall': 0.7192982456140351,\n","  'support': 57},\n"," 'English Setter': {'f1-score': 0.8870967741935485,\n","  'precision': 0.873015873015873,\n","  'recall': 0.9016393442622951,\n","  'support': 61},\n"," 'English Springer Spaniel': {'f1-score': 0.8547008547008546,\n","  'precision': 0.8620689655172413,\n","  'recall': 0.847457627118644,\n","  'support': 59},\n"," 'EntleBucher': {'f1-score': 0.8629441624365481,\n","  'precision': 0.8947368421052632,\n","  'recall': 0.8333333333333334,\n","  'support': 102},\n"," 'Eskimo Dog': {'f1-score': 0.4628099173553719,\n","  'precision': 0.39436619718309857,\n","  'recall': 0.56,\n","  'support': 50},\n"," 'Flat-coated Retriever': {'f1-score': 0.7850467289719626,\n","  'precision': 0.7636363636363637,\n","  'recall': 0.8076923076923077,\n","  'support': 52},\n"," 'French Bulldog': {'f1-score': 0.8032786885245902,\n","  'precision': 0.7777777777777778,\n","  'recall': 0.8305084745762712,\n","  'support': 59},\n"," 'German Shepard': {'f1-score': 0.7835051546391751,\n","  'precision': 0.8444444444444444,\n","  'recall': 0.7307692307692307,\n","  'support': 52},\n"," 'German Short-haired Pointer': {'f1-score': 0.8461538461538461,\n","  'precision': 0.8461538461538461,\n","  'recall': 0.8461538461538461,\n","  'support': 52},\n"," 'Giant Schnauzer': {'f1-score': 0.7636363636363637,\n","  'precision': 0.7924528301886793,\n","  'recall': 0.7368421052631579,\n","  'support': 57},\n"," 'Golden Retriever': {'f1-score': 0.8453608247422681,\n","  'precision': 0.8723404255319149,\n","  'recall': 0.82,\n","  'support': 50},\n"," 'Gordon Setter': {'f1-score': 0.8952380952380953,\n","  'precision': 0.9038461538461539,\n","  'recall': 0.8867924528301887,\n","  'support': 53},\n"," 'Great Dane': {'f1-score': 0.8679245283018867,\n","  'precision': 0.92,\n","  'recall': 0.8214285714285714,\n","  'support': 56},\n"," 'Great Pyrenees': {'f1-score': 0.8520179372197311,\n","  'precision': 0.8636363636363636,\n","  'recall': 0.8407079646017699,\n","  'support': 113},\n"," 'Greater Swiss Mountain Dog': {'f1-score': 0.8285714285714286,\n","  'precision': 0.8055555555555556,\n","  'recall': 0.8529411764705882,\n","  'support': 68},\n"," 'Groenendael': {'f1-score': 0.8431372549019608,\n","  'precision': 0.8269230769230769,\n","  'recall': 0.86,\n","  'support': 50},\n"," 'Ibizian Hound': {'f1-score': 0.8926553672316383,\n","  'precision': 0.8876404494382022,\n","  'recall': 0.8977272727272727,\n","  'support': 88},\n"," 'Irish Setter': {'f1-score': 0.920353982300885,\n","  'precision': 0.896551724137931,\n","  'recall': 0.9454545454545454,\n","  'support': 55},\n"," 'Irish Terrier': {'f1-score': 0.8396946564885496,\n","  'precision': 0.8870967741935484,\n","  'recall': 0.7971014492753623,\n","  'support': 69},\n"," 'Irish Water Spaniel': {'f1-score': 0.8910891089108911,\n","  'precision': 0.8823529411764706,\n","  'recall': 0.9,\n","  'support': 50},\n"," 'Irish Wolfhound': {'f1-score': 0.8347826086956522,\n","  'precision': 0.8571428571428571,\n","  'recall': 0.8135593220338984,\n","  'support': 118},\n"," 'Italian Greyhound': {'f1-score': 0.8342857142857142,\n","  'precision': 0.7849462365591398,\n","  'recall': 0.8902439024390244,\n","  'support': 82},\n"," 'Japanese Spaniel': {'f1-score': 0.8484848484848485,\n","  'precision': 0.875,\n","  'recall': 0.8235294117647058,\n","  'support': 85},\n"," 'Keeshond': {'f1-score': 0.9586776859504132,\n","  'precision': 0.9206349206349206,\n","  'recall': 1.0,\n","  'support': 58},\n"," 'Kelpie': {'f1-score': 0.7818181818181817,\n","  'precision': 0.7543859649122807,\n","  'recall': 0.8113207547169812,\n","  'support': 53},\n"," 'Kerry Blue Terrier': {'f1-score': 0.8774193548387097,\n","  'precision': 0.8947368421052632,\n","  'recall': 0.8607594936708861,\n","  'support': 79},\n"," 'Komondor': {'f1-score': 0.8807339449541284,\n","  'precision': 0.8727272727272727,\n","  'recall': 0.8888888888888888,\n","  'support': 54},\n"," 'Kuvasz': {'f1-score': 0.7663551401869159,\n","  'precision': 0.7192982456140351,\n","  'recall': 0.82,\n","  'support': 50},\n"," 'Labrador Retriever': {'f1-score': 0.8613138686131386,\n","  'precision': 0.8939393939393939,\n","  'recall': 0.8309859154929577,\n","  'support': 71},\n"," 'Lakeland Terrier': {'f1-score': 0.8376963350785339,\n","  'precision': 0.851063829787234,\n","  'recall': 0.8247422680412371,\n","  'support': 97},\n"," 'Leonberg': {'f1-score': 0.958904109589041,\n","  'precision': 0.963302752293578,\n","  'recall': 0.9545454545454546,\n","  'support': 110},\n"," 'Lhasa': {'f1-score': 0.7093023255813953,\n","  'precision': 0.7093023255813954,\n","  'recall': 0.7093023255813954,\n","  'support': 86},\n"," 'Malamute': {'f1-score': 0.8289473684210527,\n","  'precision': 0.8513513513513513,\n","  'recall': 0.8076923076923077,\n","  'support': 78},\n"," 'Malinois': {'f1-score': 0.8571428571428571,\n","  'precision': 0.7741935483870968,\n","  'recall': 0.96,\n","  'support': 50},\n"," 'Maltese Dog': {'f1-score': 0.8989547038327526,\n","  'precision': 0.9555555555555556,\n","  'recall': 0.8486842105263158,\n","  'support': 152},\n"," 'Mexican Hairless': {'f1-score': 0.9285714285714285,\n","  'precision': 0.9122807017543859,\n","  'recall': 0.9454545454545454,\n","  'support': 55},\n"," 'Miniature Pinscher': {'f1-score': 0.860759493670886,\n","  'precision': 0.918918918918919,\n","  'recall': 0.8095238095238095,\n","  'support': 84},\n"," 'Miniature Poodle': {'f1-score': 0.6218487394957982,\n","  'precision': 0.578125,\n","  'recall': 0.6727272727272727,\n","  'support': 55},\n"," 'Miniature Schnauzer': {'f1-score': 0.8256880733944955,\n","  'precision': 0.8181818181818182,\n","  'recall': 0.8333333333333334,\n","  'support': 54},\n"," 'Newfoundland': {'f1-score': 0.8571428571428572,\n","  'precision': 0.8617021276595744,\n","  'recall': 0.8526315789473684,\n","  'support': 95},\n"," 'Norfolk Terrier': {'f1-score': 0.7417218543046357,\n","  'precision': 0.7088607594936709,\n","  'recall': 0.7777777777777778,\n","  'support': 72},\n"," 'Norwegian Elkhound': {'f1-score': 0.8947368421052632,\n","  'precision': 0.9042553191489362,\n","  'recall': 0.8854166666666666,\n","  'support': 96},\n"," 'Norwich Terrier': {'f1-score': 0.8070175438596492,\n","  'precision': 0.8023255813953488,\n","  'recall': 0.8117647058823529,\n","  'support': 85},\n"," 'Old English Sheepdog': {'f1-score': 0.9160305343511451,\n","  'precision': 0.967741935483871,\n","  'recall': 0.8695652173913043,\n","  'support': 69},\n"," 'Otterhound': {'f1-score': 0.8108108108108107,\n","  'precision': 0.75,\n","  'recall': 0.8823529411764706,\n","  'support': 51},\n"," 'Papillon': {'f1-score': 0.9175257731958762,\n","  'precision': 0.9081632653061225,\n","  'recall': 0.9270833333333334,\n","  'support': 96},\n"," 'Pekinese': {'f1-score': 0.8659793814432989,\n","  'precision': 0.875,\n","  'recall': 0.8571428571428571,\n","  'support': 49},\n"," 'Pembroke': {'f1-score': 0.8974358974358974,\n","  'precision': 0.9333333333333333,\n","  'recall': 0.8641975308641975,\n","  'support': 81},\n"," 'Pomeranian': {'f1-score': 0.8888888888888888,\n","  'precision': 0.9043478260869565,\n","  'recall': 0.8739495798319328,\n","  'support': 119},\n"," 'Pug': {'f1-score': 0.8910891089108911,\n","  'precision': 0.8823529411764706,\n","  'recall': 0.9,\n","  'support': 100},\n"," 'Redbone': {'f1-score': 0.7222222222222223,\n","  'precision': 0.65,\n","  'recall': 0.8125,\n","  'support': 48},\n"," 'Rhodesian Ridgeback': {'f1-score': 0.7851851851851852,\n","  'precision': 0.8412698412698413,\n","  'recall': 0.7361111111111112,\n","  'support': 72},\n"," 'Rottweiler': {'f1-score': 0.8828828828828829,\n","  'precision': 0.8305084745762712,\n","  'recall': 0.9423076923076923,\n","  'support': 52},\n"," 'Saint Bernard': {'f1-score': 0.9411764705882354,\n","  'precision': 0.9696969696969697,\n","  'recall': 0.9142857142857143,\n","  'support': 70},\n"," 'Saluki': {'f1-score': 0.9015544041450777,\n","  'precision': 0.9354838709677419,\n","  'recall': 0.87,\n","  'support': 100},\n"," 'Samoyed': {'f1-score': 0.9310344827586206,\n","  'precision': 0.9473684210526315,\n","  'recall': 0.9152542372881356,\n","  'support': 118},\n"," 'Schipperke': {'f1-score': 0.9090909090909091,\n","  'precision': 0.8928571428571429,\n","  'recall': 0.9259259259259259,\n","  'support': 54},\n"," 'Scotch Terrier': {'f1-score': 0.8,\n","  'precision': 0.8461538461538461,\n","  'recall': 0.7586206896551724,\n","  'support': 58},\n"," 'Scottish Deerhound': {'f1-score': 0.9182879377431905,\n","  'precision': 0.944,\n","  'recall': 0.8939393939393939,\n","  'support': 132},\n"," 'Sealyham Terrier': {'f1-score': 0.9238578680203046,\n","  'precision': 0.9578947368421052,\n","  'recall': 0.8921568627450981,\n","  'support': 102},\n"," 'Shetland Sheepdog': {'f1-score': 0.864406779661017,\n","  'precision': 0.8360655737704918,\n","  'recall': 0.8947368421052632,\n","  'support': 57},\n"," 'Shih-Tzu': {'f1-score': 0.7822222222222222,\n","  'precision': 0.7927927927927928,\n","  'recall': 0.7719298245614035,\n","  'support': 114},\n"," 'Siberian Husky': {'f1-score': 0.7065868263473053,\n","  'precision': 0.7866666666666666,\n","  'recall': 0.6413043478260869,\n","  'support': 92},\n"," 'Silky Terrier': {'f1-score': 0.8624999999999999,\n","  'precision': 0.8961038961038961,\n","  'recall': 0.8313253012048193,\n","  'support': 83},\n"," 'Soft-coated Wheaten Terrier': {'f1-score': 0.8037383177570093,\n","  'precision': 0.8431372549019608,\n","  'recall': 0.7678571428571429,\n","  'support': 56},\n"," 'Staffordshire Bullterrier': {'f1-score': 0.6782608695652174,\n","  'precision': 0.65,\n","  'recall': 0.7090909090909091,\n","  'support': 55},\n"," 'Standard Poodle': {'f1-score': 0.7768595041322315,\n","  'precision': 0.7580645161290323,\n","  'recall': 0.7966101694915254,\n","  'support': 59},\n"," 'Standard Schnauzer': {'f1-score': 0.7090909090909091,\n","  'precision': 0.7090909090909091,\n","  'recall': 0.7090909090909091,\n","  'support': 55},\n"," 'Sussex Spaniel': {'f1-score': 0.9108910891089109,\n","  'precision': 0.92,\n","  'recall': 0.9019607843137255,\n","  'support': 51},\n"," 'Tibetan Mastiff': {'f1-score': 0.9056603773584906,\n","  'precision': 0.8888888888888888,\n","  'recall': 0.9230769230769231,\n","  'support': 52},\n"," 'Tibetan Terrier': {'f1-score': 0.832535885167464,\n","  'precision': 0.8446601941747572,\n","  'recall': 0.8207547169811321,\n","  'support': 106},\n"," 'Toy Poodle': {'f1-score': 0.7000000000000001,\n","  'precision': 0.7142857142857143,\n","  'recall': 0.6862745098039216,\n","  'support': 51},\n"," 'Toy Terrier': {'f1-score': 0.8053691275167787,\n","  'precision': 0.7792207792207793,\n","  'recall': 0.8333333333333334,\n","  'support': 72},\n"," 'Vizsla': {'f1-score': 0.8547008547008547,\n","  'precision': 0.7936507936507936,\n","  'recall': 0.9259259259259259,\n","  'support': 54},\n"," 'Walker Hound': {'f1-score': 0.5546218487394957,\n","  'precision': 0.5,\n","  'recall': 0.6226415094339622,\n","  'support': 53},\n"," 'Weimaraner': {'f1-score': 0.8925619834710743,\n","  'precision': 0.8852459016393442,\n","  'recall': 0.9,\n","  'support': 60},\n"," 'Welsh Springer Spaniel': {'f1-score': 0.8,\n","  'precision': 0.8444444444444444,\n","  'recall': 0.76,\n","  'support': 50},\n"," 'West Highland White Terrier': {'f1-score': 0.8345323741007195,\n","  'precision': 0.8285714285714286,\n","  'recall': 0.8405797101449275,\n","  'support': 69},\n"," 'Whippet': {'f1-score': 0.8263473053892216,\n","  'precision': 0.8625,\n","  'recall': 0.7931034482758621,\n","  'support': 87},\n"," 'Wirehaired Fox Terrier': {'f1-score': 0.7642276422764227,\n","  'precision': 0.7121212121212122,\n","  'recall': 0.8245614035087719,\n","  'support': 57},\n"," 'Yorkshire Terrier': {'f1-score': 0.7538461538461539,\n","  'precision': 0.7424242424242424,\n","  'recall': 0.765625,\n","  'support': 64},\n"," 'accuracy': 0.8441724941724942,\n"," 'macro avg': {'f1-score': 0.8362678710724345,\n","  'precision': 0.8356394159112934,\n","  'recall': 0.8404657114347475,\n","  'support': 8580},\n"," 'weighted avg': {'f1-score': 0.8451945734106948,\n","  'precision': 0.8493963287450592,\n","  'recall': 0.8441724941724942,\n","  'support': 8580}}"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"code","metadata":{"id":"T3y0AsXdEBII","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"f9217674-9b25-4fb2-a6a6-16b1f4a72f4e","executionInfo":{"status":"ok","timestamp":1588300126053,"user_tz":240,"elapsed":883,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["with open(\"classification_report_short.txt\", \"r\") as f:\n","  text = f.read()\n","\n","print(text)"],"execution_count":96,"outputs":[{"output_type":"stream","text":["                                precision    recall  f1-score   support\n","\n","                      Chihuaha       0.71      0.87      0.78        52\n","              Japanese Spaniel       0.88      0.82      0.85        85\n","                   Maltese Dog       0.96      0.85      0.90       152\n","                      Pekinese       0.88      0.86      0.87        49\n","                      Shih-Tzu       0.79      0.77      0.78       114\n","                                     ..............\n","                                     ..............\n","                                     ..............\n","               Standard Poodle       0.76      0.80      0.78        59\n","              Mexican Hairless       0.91      0.95      0.93        55\n","                         Dingo       0.73      0.84      0.78        56\n","                         Dhole       0.90      0.90      0.90        50\n","           African Hunting Dog       0.91      0.97      0.94        69\n","\n","                      accuracy                           0.84      8580\n","                     macro avg       0.84      0.84      0.84      8580\n","                  weighted avg       0.85      0.84      0.85      8580\n","\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ScratchModel.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gnlAS3f7IASO","colab_type":"code","colab":{}},"source":["from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","\n","plt.ion()   # interactive mode\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMoiwQKvz9vp","colab_type":"code","colab":{}},"source":["data_dir = \"data/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QAqpRnqRz-P5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"66ae065e-1e5c-4c1c-d588-9377840bd230","executionInfo":{"status":"ok","timestamp":1588196256997,"user_tz":240,"elapsed":4164,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["from data.load_data import load_datasets\n","train_dataset, val_dataset, test_dataset, classes = load_datasets(data_dir)\n","class_names = train_dataset.classes\n","print(len(class_names))\n","\n","dataset_sizes = {\"train\": len(train_dataset), \"val\": len(val_dataset)}"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Stanford Dogs Dataset already downloaded\n","Training set stats:\n","10306 samples spanning 120 classes (avg 85.883333 per class)\n","Validation set stats:\n","2571 samples spanning 120 classes (avg 21.425000 per class)\n","Testing set stats:\n","9249 samples spanning 120 classes (avg 77.075000 per class)\n","120\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h2ViGCdmz_w4","colab_type":"code","colab":{}},"source":["kwargs = {'num_workers': 1, 'pin_memory': True}\n","batch_size = 32\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                 batch_size=batch_size, shuffle=True, **kwargs)\n","dataloaders = {\"train\": train_loader, \"val\": val_loader}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfvhq1py0Bm-","colab_type":"code","colab":{}},"source":["def evaluate(model, optimizer, criterion):\n","    model.eval()\n","    running_loss = 0\n","    running_corrects = 0\n","    n_examples = 0\n","    with torch.no_grad():\n","        \n","        model.eval()   # Set model to evaluate mode\n","\n","        for inputs, labels in test_loader:\n","        \n","          inputs = inputs.to(device)\n","          # inputs = inputs.cuda()\n","          labels = labels.to(device)\n","          # labels = labels.cuda()\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          # forward\n","          # track history if only in train\n","          with torch.set_grad_enabled(False):\n","              outputs = model(inputs)\n","              _, preds = torch.max(outputs, 1)\n","              loss = criterion(outputs, labels)\n","\n","          # statistics\n","          running_loss += loss.item() * inputs.size(0)\n","          running_corrects += torch.sum(preds == labels.data)\n","          n_examples += preds.size(0)\n","\n","    epoch_loss = running_loss / len(test_dataset)\n","    epoch_acc = 100. * running_corrects.double() / len(test_dataset)\n","    running_loss /= n_examples\n","    return running_loss, running_corrects, n_examples, epoch_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNX_lkIL0DpI","colab_type":"code","colab":{}},"source":["def train_model(model, model_name, criterion, optimizer, scheduler, hp_info, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    file_name = \"{model_name}_lr_{lr}_mom_{momentum}.log\"\n","    file_name = file_name.format(model_name=model_name, lr=hp_info['lr'], momentum=hp_info['momentum'])\n","    log_file = open(file_name, 'w')\n","    \n","    print('-'*10)\n","    print('learning rate: {}, momentum: {}'.format(hp_info['lr'], hp_info['momentum']))\n","\n","    for epoch in range(num_epochs):\n","        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        # print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        log_file_string = \"\"\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                # inputs = inputs.cuda()\n","                labels = labels.to(device)\n","                # labels = labels.cuda()\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            \n","            if phase == 'train':\n","                log_file_string = 'Train Epoch: {}\\t Train Loss: {:.6f}\\t Train Acc:{}\\t '.format(epoch, epoch_loss, epoch_acc)\n","            else:\n","                log_file_string += 'Val Loss: {}\\t Val Acc: {}\\n'.format(epoch_loss, epoch_acc)\n","                log_file.write(log_file_string)\n","                print(log_file_string)\n","            # log_file.write(log_file_string)\n","            # print(log_file_string)\n","\n","            # print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","            #     phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        # print()\n","\n","    time_elapsed = time.time() - since\n","    # print('Training complete in {:.0f}m {:.0f}s'.format(\n","    #     time_elapsed // 60, time_elapsed % 60))\n","    # print('Best val Acc: {:4f}'.format(best_acc))  \n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    test_loss, test_correct, test_n_examples, test_acc = evaluate(model, optimizer, criterion)\n","    log_file_string = '\\ntest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n","    log_file_string = log_file_string.format(test_loss, test_correct, test_n_examples, test_acc)\n","    log_file.write(log_file_string)\n","    print(log_file_string)\n","\n","    log_file.close()\n","\n","    return model, test_acc, best_acc\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tzvUaX00GFH","colab_type":"code","colab":{}},"source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Khs21gEl0IiB","colab_type":"code","colab":{}},"source":["class BasicCNN(nn.Module):\n","  def __init__(self, num_classes):\n","    super(BasicCNN, self).__init__()\n","    channels_in = 3 # RGB Image\n","    self.conv_layer1 = nn.Sequential(\n","        nn.Conv2d(3, 32, 4),\n","        nn.BatchNorm2d(32),\n","        nn.ReLU(inplace=True),\n","        nn.AvgPool2d(2, 2),\n","        nn.Dropout(p=0.1)\n","    )\n","    self.conv_layer2 = nn.Sequential(\n","        nn.Conv2d(32, 64, 3),\n","        nn.BatchNorm2d(64),\n","        nn.ReLU(inplace=True),\n","        nn.AvgPool2d(2, 2),\n","        nn.Dropout(p=0.1)\n","    )\n","    self.conv_layer3 = nn.Sequential(\n","        nn.Conv2d(64, 128, 3),\n","        nn.BatchNorm2d(128),\n","        nn.ReLU(inplace=True),\n","        nn.AvgPool2d(2, 2),\n","        nn.Dropout(p=0.1)\n","    )\n","    self.max_pool1 = nn.MaxPool2d(2, 2)\n","    self.conv_layer4 = nn.Sequential(\n","        nn.Conv2d(128, 256, 3),\n","        nn.BatchNorm2d(256),\n","        nn.ReLU(inplace=True),\n","        nn.AvgPool2d(2, 2),\n","        nn.Dropout(p=0.1)\n","    )\n","    self.max_pool2 = nn.MaxPool2d(2, 2)\n","\n","    self.fc1 = nn.Linear(1024, 512)\n","    self.fc2 = nn.Linear(512, num_classes)\n","\n","  def forward(self, x):\n","    # print(x.size())\n","\n","    x = self.conv_layer1(x)\n","    x = self.conv_layer2(x)\n","    x = self.conv_layer3(x)\n","    x = self.max_pool1(x)\n","    x = self.conv_layer4(x)\n","    x = self.max_pool2(x)\n","    \n","    x = x.view(x.size(0), -1)\n","    out = self.fc2(self.fc1(x))\n","\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKLeD6ba0v3A","colab_type":"code","colab":{}},"source":["learning_rates = [0.001, 0.01, 0.1]\n","momentums = [0.99, 0.9, 0.5]\n","\n","model_type = \"basicmodel\"\n","my_models = []\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","for lr in learning_rates:\n","    for mom in momentums:\n","      model_conv = BasicCNN(120)\n","      model_conv = model_conv.to(device)\n","\n","      params = model_conv.parameters()\n","      optimizer_conv = optim.SGD(params, lr=lr, momentum=mom)\n","\n","      # Decay LR by a factor of 0.1 every 7 epochs\n","      exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n","      hp_info = {'lr': lr, 'momentum': mom}\n","      my_models.append({'model': model_conv, 'optimizer': optimizer_conv, 'exp_lr_scheduler': exp_lr_scheduler, 'hp_info': hp_info})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISF0v1fs05x4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"747b4ab5-8720-4788-e7bd-d92195859534","executionInfo":{"status":"ok","timestamp":1588203602726,"user_tz":240,"elapsed":7215000,"user":{"displayName":"Robert Firstman","photoUrl":"","userId":"09026127748730206662"}}},"source":["# model_conv = train_model(model_conv, criterion, optimizer_conv,\n","#                          exp_lr_scheduler, num_epochs=25)\n","\n","best_test_acc = 0\n","best_val_acc = 0\n","best_model = copy.deepcopy(my_models[0]['model'].state_dict())\n","best_model_hp_info = copy.deepcopy(my_models[0]['hp_info'])\n","for model_i in my_models:\n","    model, test_acc, val_acc = train_model(model_i['model'],\n","                                           model_type, \n","                                           criterion, \n","                                           model_i['optimizer'],\n","                                           model_i['exp_lr_scheduler'], \n","                                           model_i['hp_info'], \n","                                           num_epochs=10)\n","    if test_acc > best_test_acc:\n","        best_test_acc = test_acc\n","        best_val_acc = val_acc\n","        best_model = copy.deepcopy(model.state_dict())\n","        best_model_hp_info = copy.deepcopy(model_i['hp_info'])\n","\n","model_conv = best_model"],"execution_count":20,"outputs":[{"output_type":"stream","text":["----------\n","learning rate: 0.001, momentum: 0.99\n","Train Epoch: 0\t Train Loss: 4.651094\t Train Acc:0.025033960799534252\t Val Loss: 4.521221354331362\t Val Acc: 0.034227926876701675\n","\n","Train Epoch: 1\t Train Loss: 4.472383\t Train Acc:0.0392004657481079\t Val Loss: 4.429705133701474\t Val Acc: 0.041229093737845196\n","\n","Train Epoch: 2\t Train Loss: 4.400091\t Train Acc:0.045022317096836795\t Val Loss: 4.370075723996009\t Val Acc: 0.044340723453908985\n","\n","Train Epoch: 3\t Train Loss: 4.332304\t Train Acc:0.0504560450223171\t Val Loss: 4.329779992721184\t Val Acc: 0.051341890315052506\n","\n","Train Epoch: 4\t Train Loss: 4.270853\t Train Acc:0.05598680380360955\t Val Loss: 4.270765565889255\t Val Acc: 0.056009334889148193\n","\n","Train Epoch: 5\t Train Loss: 4.229456\t Train Acc:0.06452551911507859\t Val Loss: 4.237419726643661\t Val Acc: 0.055231427460132244\n","\n","Train Epoch: 6\t Train Loss: 4.129649\t Train Acc:0.07151174073355326\t Val Loss: 4.250464628569025\t Val Acc: 0.06223259432127577\n","\n","Train Epoch: 7\t Train Loss: 4.032067\t Train Acc:0.09111197360760723\t Val Loss: 4.102331771130972\t Val Acc: 0.07429015947102295\n","\n","Train Epoch: 8\t Train Loss: 3.988662\t Train Acc:0.10130021346788279\t Val Loss: 4.1019106944309325\t Val Acc: 0.08206923376118241\n","\n","Train Epoch: 9\t Train Loss: 3.974210\t Train Acc:0.10003881234232487\t Val Loss: 4.079611275642125\t Val Acc: 0.09529366005445353\n","\n","\n","test set: Average loss: 4.0427, Accuracy: 843/9249 (9%)\n","\n","----------\n","learning rate: 0.001, momentum: 0.9\n","Train Epoch: 0\t Train Loss: 4.580131\t Train Acc:0.025519115078594994\t Val Loss: 4.4724028894104695\t Val Acc: 0.02956048230260599\n","\n","Train Epoch: 1\t Train Loss: 4.394093\t Train Acc:0.03793906462254997\t Val Loss: 4.402073984617206\t Val Acc: 0.0470633994554648\n","\n","Train Epoch: 2\t Train Loss: 4.331141\t Train Acc:0.04938870560838347\t Val Loss: 4.360746273870182\t Val Acc: 0.049397121742512644\n","\n","Train Epoch: 3\t Train Loss: 4.262989\t Train Acc:0.06142053172908985\t Val Loss: 4.320619631536922\t Val Acc: 0.05484247374562427\n","\n","Train Epoch: 4\t Train Loss: 4.213846\t Train Acc:0.06520473510576363\t Val Loss: 4.350915701553451\t Val Acc: 0.05250875145857643\n","\n","Train Epoch: 5\t Train Loss: 4.161562\t Train Acc:0.07287017271492335\t Val Loss: 4.657640398731031\t Val Acc: 0.03500583430571762\n","\n","Train Epoch: 6\t Train Loss: 4.121341\t Train Acc:0.08131185717058025\t Val Loss: 4.221140819595557\t Val Acc: 0.06378840917930766\n","\n","Train Epoch: 7\t Train Loss: 4.033778\t Train Acc:0.09373180671453522\t Val Loss: 4.165992626092259\t Val Acc: 0.07429015947102295\n","\n","Train Epoch: 8\t Train Loss: 4.013272\t Train Acc:0.09984475063070056\t Val Loss: 4.201226692911902\t Val Acc: 0.07117852975495916\n","\n","Train Epoch: 9\t Train Loss: 3.996443\t Train Acc:0.1033378614399379\t Val Loss: 4.171329086552979\t Val Acc: 0.06923376118241929\n","\n","\n","test set: Average loss: 4.1405, Accuracy: 717/9249 (8%)\n","\n","----------\n","learning rate: 0.001, momentum: 0.5\n","Train Epoch: 0\t Train Loss: 4.725827\t Train Acc:0.01892101688336891\t Val Loss: 4.6439837413462906\t Val Acc: 0.03500583430571762\n","\n","Train Epoch: 1\t Train Loss: 4.575291\t Train Acc:0.031535028138948186\t Val Loss: 4.552015506354161\t Val Acc: 0.02956048230260599\n","\n","Train Epoch: 2\t Train Loss: 4.494901\t Train Acc:0.036677663496992044\t Val Loss: 4.50359625428718\t Val Acc: 0.037728510307273436\n","\n","Train Epoch: 3\t Train Loss: 4.441573\t Train Acc:0.03987968173879294\t Val Loss: 4.46485079684085\t Val Acc: 0.03733955659276546\n","\n","Train Epoch: 4\t Train Loss: 4.392839\t Train Acc:0.04308169998059383\t Val Loss: 4.424688993748502\t Val Acc: 0.048619214313496695\n","\n","Train Epoch: 5\t Train Loss: 4.369343\t Train Acc:0.04812730448282554\t Val Loss: 4.404780985269506\t Val Acc: 0.043173862310385065\n","\n","Train Epoch: 6\t Train Loss: 4.342206\t Train Acc:0.055695711236173104\t Val Loss: 4.392747841749336\t Val Acc: 0.043951769739401014\n","\n","Train Epoch: 7\t Train Loss: 4.316192\t Train Acc:0.05414321754317873\t Val Loss: 4.384995737598825\t Val Acc: 0.04161804745235317\n","\n","Train Epoch: 8\t Train Loss: 4.324064\t Train Acc:0.05365806326411799\t Val Loss: 4.3663191656248435\t Val Acc: 0.043173862310385065\n","\n","Train Epoch: 9\t Train Loss: 4.313795\t Train Acc:0.05414321754317873\t Val Loss: 4.355549116071584\t Val Acc: 0.048230260598988724\n","\n","\n","test set: Average loss: 4.3937, Accuracy: 485/9249 (5%)\n","\n","----------\n","learning rate: 0.01, momentum: 0.99\n","Train Epoch: 0\t Train Loss: 4.870934\t Train Acc:0.018338831748496022\t Val Loss: 4.603179893805238\t Val Acc: 0.024504084014002333\n","\n","Train Epoch: 1\t Train Loss: 4.618934\t Train Acc:0.02066757228798758\t Val Loss: 4.603402401305784\t Val Acc: 0.021781408012446518\n","\n","Train Epoch: 2\t Train Loss: 4.613612\t Train Acc:0.02289928197166699\t Val Loss: 4.602856785534236\t Val Acc: 0.021003500583430573\n","\n","Train Epoch: 3\t Train Loss: 4.625653\t Train Acc:0.025616145934407142\t Val Loss: 4.6872179156748155\t Val Acc: 0.019058732010890703\n","\n","Train Epoch: 4\t Train Loss: 4.609417\t Train Acc:0.02454880652047351\t Val Loss: 4.559914376938774\t Val Acc: 0.025281991443018282\n","\n","Train Epoch: 5\t Train Loss: 4.597979\t Train Acc:0.020279448864738987\t Val Loss: 4.561097853381269\t Val Acc: 0.023726176584986387\n","\n","Train Epoch: 6\t Train Loss: 4.575948\t Train Acc:0.02183194255773336\t Val Loss: 4.561671555436385\t Val Acc: 0.023337222870478413\n","\n","Train Epoch: 7\t Train Loss: 4.494559\t Train Acc:0.030370657869202407\t Val Loss: 4.521848831785092\t Val Acc: 0.027615713730066122\n","\n","Train Epoch: 8\t Train Loss: 4.442944\t Train Acc:0.03250533669706967\t Val Loss: 4.4796253586037995\t Val Acc: 0.026837806301050177\n","\n","Train Epoch: 9\t Train Loss: 4.450350\t Train Acc:0.03531923151562197\t Val Loss: 4.452625383527675\t Val Acc: 0.02956048230260599\n","\n","\n","test set: Average loss: 4.4390, Accuracy: 311/9249 (3%)\n","\n","----------\n","learning rate: 0.01, momentum: 0.9\n","Train Epoch: 0\t Train Loss: 4.611581\t Train Acc:0.027653793906462255\t Val Loss: 4.617803234017252\t Val Acc: 0.022559315441462467\n","\n","Train Epoch: 1\t Train Loss: 4.442076\t Train Acc:0.03745391034348923\t Val Loss: 4.410438977897701\t Val Acc: 0.032283158304161806\n","\n","Train Epoch: 2\t Train Loss: 4.351751\t Train Acc:0.04715699592470406\t Val Loss: 4.436138955779875\t Val Acc: 0.035394788020225595\n","\n","Train Epoch: 3\t Train Loss: 4.272621\t Train Acc:0.05686008150591888\t Val Loss: 4.409929279963917\t Val Acc: 0.045507584597432905\n","\n","Train Epoch: 4\t Train Loss: 4.213506\t Train Acc:0.06413739569183\t Val Loss: 4.839287789959427\t Val Acc: 0.033450019447685726\n","\n","Train Epoch: 5\t Train Loss: 4.175641\t Train Acc:0.06898893848243742\t Val Loss: 4.404944352907042\t Val Acc: 0.050563982886036564\n","\n","Train Epoch: 6\t Train Loss: 4.118058\t Train Acc:0.07743062293809432\t Val Loss: 4.2525667795547015\t Val Acc: 0.06339945546479969\n","\n","Train Epoch: 7\t Train Loss: 3.972597\t Train Acc:0.09790413351445759\t Val Loss: 4.016613834148156\t Val Acc: 0.08790353947880203\n","\n","Train Epoch: 8\t Train Loss: 3.895609\t Train Acc:0.10848049679798176\t Val Loss: 4.0071335609485\t Val Acc: 0.09373784519642163\n","\n","Train Epoch: 9\t Train Loss: 3.886483\t Train Acc:0.10721909567242383\t Val Loss: 4.021106346160788\t Val Acc: 0.09607156748346947\n","\n","\n","test set: Average loss: 3.9899, Accuracy: 888/9249 (10%)\n","\n","----------\n","learning rate: 0.01, momentum: 0.5\n","Train Epoch: 0\t Train Loss: 4.526044\t Train Acc:0.030758781292451\t Val Loss: 4.814726429289762\t Val Acc: 0.022948269155970438\n","\n","Train Epoch: 1\t Train Loss: 4.363895\t Train Acc:0.043857946827091016\t Val Loss: 4.458599121549258\t Val Acc: 0.03072734344612991\n","\n","Train Epoch: 2\t Train Loss: 4.269700\t Train Acc:0.057830390064040366\t Val Loss: 5.627242524537628\t Val Acc: 0.024504084014002333\n","\n","Train Epoch: 3\t Train Loss: 4.192134\t Train Acc:0.06481661168251504\t Val Loss: 4.659710170600443\t Val Acc: 0.024893037728510307\n","\n","Train Epoch: 4\t Train Loss: 4.129273\t Train Acc:0.07461672811954201\t Val Loss: 4.196684494782681\t Val Acc: 0.06534422403733955\n","\n","Train Epoch: 5\t Train Loss: 4.069325\t Train Acc:0.07985639433339803\t Val Loss: 4.504332053155947\t Val Acc: 0.043562816024893036\n","\n","Train Epoch: 6\t Train Loss: 4.014434\t Train Acc:0.089656510770425\t Val Loss: 4.558451749804603\t Val Acc: 0.041229093737845196\n","\n","Train Epoch: 7\t Train Loss: 3.877467\t Train Acc:0.11585484183970503\t Val Loss: 4.110423344018996\t Val Acc: 0.07779074290159471\n","\n","Train Epoch: 8\t Train Loss: 3.835834\t Train Acc:0.12478168057442267\t Val Loss: 4.011546887576186\t Val Acc: 0.0941267989109296\n","\n","Train Epoch: 9\t Train Loss: 3.822548\t Train Acc:0.12614011255579274\t Val Loss: 4.129672583138976\t Val Acc: 0.07740178918708673\n","\n","\n","test set: Average loss: 3.9631, Accuracy: 1028/9249 (11%)\n","\n","----------\n","learning rate: 0.1, momentum: 0.99\n","Train Epoch: 0\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 1\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 2\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 3\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 4\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 5\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 6\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 7\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 8\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 9\t Train Loss: nan\t Train Acc:0.008053561032408306\t Val Loss: nan\t Val Acc: 0.008556981719175419\n","\n","\n","test set: Average loss: nan, Accuracy: 53/9249 (1%)\n","\n","----------\n","learning rate: 0.1, momentum: 0.9\n","Train Epoch: 0\t Train Loss: 9.074870\t Train Acc:0.008635746167281196\t Val Loss: 5.8457601675955715\t Val Acc: 0.008168028004667444\n","\n","Train Epoch: 1\t Train Loss: 6.154725\t Train Acc:0.009800116437026975\t Val Loss: 6.251775347644\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 2\t Train Loss: 5.090961\t Train Acc:0.00785949932078401\t Val Loss: 4.815532621267193\t Val Acc: 0.0077790742901594715\n","\n","Train Epoch: 3\t Train Loss: 4.942237\t Train Acc:0.009800116437026975\t Val Loss: 5.284191334307866\t Val Acc: 0.008168028004667444\n","\n","Train Epoch: 4\t Train Loss: 4.976170\t Train Acc:0.008829807878905492\t Val Loss: 4.813454492413919\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 5\t Train Loss: 4.901860\t Train Acc:0.008635746167281196\t Val Loss: 4.861059637674327\t Val Acc: 0.010112796577207312\n","\n","Train Epoch: 6\t Train Loss: 4.874592\t Train Acc:0.009023869590529789\t Val Loss: 4.871167434787342\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 7\t Train Loss: 4.800143\t Train Acc:0.010188239860275568\t Val Loss: 4.815679573725282\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 8\t Train Loss: 4.794147\t Train Acc:0.008247622744032602\t Val Loss: 4.789864367244309\t Val Acc: 0.008556981719175419\n","\n","Train Epoch: 9\t Train Loss: 4.793026\t Train Acc:0.009314962157966233\t Val Loss: 4.792175639051262\t Val Acc: 0.008168028004667444\n","\n","\n","test set: Average loss: 4.9002, Accuracy: 160/9249 (2%)\n","\n","----------\n","learning rate: 0.1, momentum: 0.5\n","Train Epoch: 0\t Train Loss: 4.653476\t Train Acc:0.02357849796235203\t Val Loss: 5.006890859088282\t Val Acc: 0.014002333722287048\n","\n","Train Epoch: 1\t Train Loss: 4.454873\t Train Acc:0.035513293227246265\t Val Loss: 4.4760774742625005\t Val Acc: 0.026837806301050177\n","\n","Train Epoch: 2\t Train Loss: 4.369190\t Train Acc:0.043275761692218126\t Val Loss: 4.641511985737264\t Val Acc: 0.027226760015558148\n","\n","Train Epoch: 3\t Train Loss: 4.300843\t Train Acc:0.05414321754317873\t Val Loss: 4.928761812927472\t Val Acc: 0.027615713730066122\n","\n","Train Epoch: 4\t Train Loss: 4.231903\t Train Acc:0.06035319231515622\t Val Loss: 5.635787485175354\t Val Acc: 0.023726176584986387\n","\n","Train Epoch: 5\t Train Loss: 4.195416\t Train Acc:0.06501067339413934\t Val Loss: 4.856008398537504\t Val Acc: 0.021781408012446518\n","\n","Train Epoch: 6\t Train Loss: 4.130315\t Train Acc:0.07393751212885698\t Val Loss: 4.319347084101793\t Val Acc: 0.051341890315052506\n","\n","Train Epoch: 7\t Train Loss: 4.010589\t Train Acc:0.0952843004075296\t Val Loss: 4.102219252120841\t Val Acc: 0.08479190976273823\n","\n","Train Epoch: 8\t Train Loss: 3.943101\t Train Acc:0.10498738598874442\t Val Loss: 4.450456792303951\t Val Acc: 0.049786075457020615\n","\n","Train Epoch: 9\t Train Loss: 3.922950\t Train Acc:0.1087715893654182\t Val Loss: 4.004451057874753\t Val Acc: 0.09529366005445353\n","\n","\n","test set: Average loss: 3.9738, Accuracy: 907/9249 (10%)\n","\n"],"name":"stdout"}]}]}